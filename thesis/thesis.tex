\documentclass[a4paper,11pt]{kth-mag}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage[swedish,english]{babel}
\usepackage{modifications}
\usepackage[backend=biber]{biblatex}
\usepackage{todonotes}
\usepackage{listings}

\lstset{%
  basicstyle=\footnotesize,
  numbers=left,
  frame=l
}

\bibliography{bibliography.bib}

\newenvironment{metatext}{%
  \textbf{$\hookrightarrow$}
  \begin{itshape}
}{
  \end{itshape}
  \newline
  \newline
  \useignorespacesandallpars
}

\def\useignorespacesandallpars#1\ignorespaces\fi{%
#1\fi\ignorespacesandallpars}

\makeatletter
\def\ignorespacesandallpars{%
  \@ifnextchar\par
    {\expandafter\ignorespacesandallpars\@gobble}%
    {}%
}
\makeatother

\title{Modular responsive web design}
\foreigntitle{Modulär responsiv webbutveckling}
\subtitle{Allowing responsive web modules to respond to custom criterias instead of only viewport size by implementing \emph{element queries}}
\author{Lucas Wiener \\ \lowercase{lwiener@kth.se}}
\date{February 2015}
\blurb{Master's Thesis at \textsc{csc}\\\hfill\\ Supervisors at \textsc{evry ab}: Tomas Ekholm \& Stefan Sennerö\\Supervisor at \textsc{csc}: Philipp Haller\\Examiner: Mads Dam}
\trita{TRITA xxx yyyy-nn}
\begin{document}
  \frontmatter
  \pagestyle{empty}
  \removepagenumbers
  \maketitle
  \selectlanguage{english}
  \begin{abstract}
    Abstract goes here.
  \end{abstract}
  \clearpage
  \begin{foreignabstract}{swedish}
    Sammanfattning ska vara här.
  \end{foreignabstract}
  \clearpage
  \tableofcontents*
  \mainmatter
  \pagestyle{newchap}
  \chapter{Introduction}
    \section{Targeted audience}
    \section{Problem statement}
      By using \textsc{css} \emph{media queries} developers can specify different rules for different viewport sizes.
      This is fundamental to create responsive web applications.
      If developers want to build modular applications by composing the application by smaller components (elements, scripts, styles, etc.) media queries are no longer applicable.
      Modular components should be able to react and change style depending on the given size that the component has been given by the application, not the viewport size.
      The problem can be formulated as: \emph{Elements can not specify conditional style rules depending on their own size, or the size of any other element}.
      See the included document in section~\ref{sec:problem-formulation} of the appendix for a more practical problem formulation.

      The main international standards organization for the World Wide Web (World Wide Web Consortium, abbreviated \textsc{w3c}) has unofficially stated that such feature would be infeasible to implement.
      Some problems with implementing element queries in \textsc{css} are:
      \begin{itemize}
        \item \textbf{Circularity:} The styling of elements depend on many factors (theoretically on all other elements in the layout tree).
        If elements can apply styles by criterias of other elements, it will be possible to create infinite loops of styling.
        The simplest example of this would be an element to set its width to 200 pixels if it is under 100 pixels wide.
        If the element is under 100 pixels wide, the new style will be applied to the element which would make the width of the element 200 pixels.
        If this element would have another rule that set its width to 50 pixels if it is wider than 150 pixels, there is an infinite loop of styling.
        Problems like this can probably be catched during \textsc{css} parsing, but there are so many combinations of style properties that could result in similar loops that it will add a lot of complexity to the language, both for implementers and users.
        See the included document in section~\ref{sec:cyclic-rules} of the appendix for more examples and deeper discussions about cyclic rules.
        \item \textbf{Performance:} layout engines typically perform selector matching and layout computations in parallel to achieve good performance.
        If element queries would be implemented, the layout engines would need to first compute the layout of all elements in order to decide which selectors would conform to the element query conditions and then do a new layout computation, and so on until a stable state has been reached.
        Far worse, since selectors now depend on layout style, this cannot be done in parallel which impacts performance heavily.
      \end{itemize}
      Because of the problems, it is stated that such feature will not be implemented in the near future. So it is now up to the developers to implement this feature as a third-party solution. 
      Efforts have been made by big players to create a robust implementation, with moderate success. 
      Since all implementations have shortcomings, there is still no de facto solution that developers use and the problem remains unsolved.

    \section{Objective}
      The main objective of this thesis is to develop a third-party implementation of element queries (or equivalent to solve the problem of modular responsive elements).
      To do this, it is needed to research existing implementation attempts in order to understand and analyze the advantages and shortcomings of the approaches.
      It is also neccesary to be aware of the premises, such as browser limitations and specifications that need to be conformed.
      In addition, research will be done about the problems of implementing element queries natively, to get a deeper understanding of how an official \textsc{api} would look like.
      There are many challenges along the way that will need to be researched and worked around.
      Examples of such subproblems that would need to be investigated are:
      \begin{itemize}
        \item How should circularity be handled? Should it be detected at runtime or parsetime, and what should happen on detection?
        \item How can one listen to element dimension changes without any native support?
        \item How can a custom \textsc{api} be crafted that will enable element queries and still conform to the \textsc{css} specification?
        \item If a custom \textsc{api} is developed, how would one make third-party modules (that uses media queries) work without demanding a rewrite of all third-party modules?
      \end{itemize}
      The scientific question to be answered is if it is possible to solve the problem  without extending the current web standard.
      The hypothesis is that the problem can be solved in a reliable and performant way by crafting a third-party implementation.
      A reliable implementation should also enable existing responsive components to react to a specified criteria (parent container size for example) with no modifications to the components.
      The goal of the thesis should be considered fulfilled if a solution was successfully implemented or described, or if the problems hindering a solution are thoroughly documented.


    \section{Significance}
      Many frameworks and techniques are being used in web development to keep the code from becoming an entangled mess.
      Creating modules helps to ease development and increase reusability.
      One of the biggest issues that keeps web modules from being truly modular is that they cannot adapt to given sizes.
      This makes the modules either force the client to style them properly depending on viewport size, or not being responsive.
      Both options are undesirable for developing larger applications.
      A third option would be to make the modules context aware and style themselves according to the viewport, which defeats the purpose of modules (making them not reusable).

      The last couple of years a lot of articles have been written about the problem and how badly we need element queries.
      As already stated, third-party implementation efforts have been made by small and big players, with moderate success.
      \textsc{w3c} keep getting requests and questions about it, but the answer seems to lean towards no.
      An organization called Responsive Issues Community Group (abbreviated \textsc{ricg}) have started an initial planning regarding element queries.
      However, things are moving slow and a draft about element queries use cases are still being made.

      Solving this problem would be a big advancement to web development, enabling developers to create truly modular components.
      By studying the problem, identifying approaches and providing a third-party solution the community can take a step closer to solve the problem.
      If the hypothesis holds, developers will be able to use element queries in the near future, while waiting for \textsc{w3c} to make their verdict.
      The outcome of this thesis can also be helpful for \textsc{w3c} and others to get an overview of the problem and possibly get ideas how subproblems can be handled.
    \section{Methodology}
      \todo{What should go here?}
    \section{Delimitations}
      The focus of the thesis lays on developing a third-party framework that realizes element queries.
      All theoretical studies and work will be performed to support the development of the framework.

      \subsection{What will be done}
      \begin{itemize}
      \item A third-party implementation of element queries will be developed.
      \item The problems of implementing element queries natively will be addressed.
      \item Theory about layout engines, \textsc{css}, \textsc{html} and responsive web design will be given to fully understand the problem.
      \end{itemize}

      \subsection{What will not be done}
      \begin{itemize}
      \item No efforts will be made to solve the problems accompanied with a native solution.
      \item No \textsc{api} or similar will be designed for a native solution.
      \item \textsc{ui} and \textsc{ux} design will not be addressed, other than necessary for understanding the problem.
      \item No complete history of browsers, the Internet or responsive web design will be given other than neccesary.
      \end{itemize}
    \section{Outline}

  \part{Background}

    \chapter{Browsers} 
      \begin{metatext}
        Browsers and the Internet is something that many people today take for granted.
        It is not longer the case that only computer scientists are browsing the web.
        Today the web is becoming increasingly important in both our personal and professional lives.
        This chapter will give a brief history of browsers and the rise of the web.
        This section is a summary of \normalfont{\cite{internet_live_stats}\cite{internet_of_things}\cite{wiki_hypermedia}\cite{w3c_www}\cite{oed}}.
      \end{metatext}

      Before addressing the birth of the web, lets define the meaning of the concepets of the \emph{Internet}, \emph{Web} and \emph{World Wide Web}.
      The word internet can be translated to \emph{something between networks}. 
      When referring to the Internet (capitalized) it is usually the global decentralized internet used for communication between millions of networks using \textsc{tcp/ip}.
      Since the Internet is decentralized, there is no owner.
      Or in other words, the owners are all the network end-points which means all users of the Internet.
      One can argue that the owners of the Internet are the \textsc{isp}'s, providing the services and infrastructure making the Internet possible.
      On the other hand, the backbones of the Internet are usually co-founded by countries and companies.
      Or is it the \textsc{icann}\footnote{The Internet Corporation for Assigned Names and Numbers} organization which has the responsibility for managing the \textsc{ip} addresses in the Internet namespace?
      Clearly, the Internet wouldn't be what it is today without all the actors.
      The Internet lays the ground for many systems and applications, including the World Wide Web, file sharing and telephony.
      In 2014 the number of Internet users was measured to just below 3 billions, and estimations shows that we have surpassed 3 billions users today (no report for 2015 has been made yet).
      Users are here defined as humans having unrestricted acccess to the Internet.
      If one instead measures the number of connected entities (electronic devices that communicates through the Internet) the numbers are much higher.
      An estimation for 2015 of 25 billions connected entities has been made, and the estimation for 2020 is 50 billions.
      
      As already stated, the Word Wide Web (abbreviated \textsc{www} or \textsc{w3}) is a system that operates through the Internet.
      The World Wide Web is usually shortened to simply \emph{the web}.
      The web is a system for accessing interlinked hypertext documents and other media such as images and videos.
      Since not only hypertext is interlinked on the web, the term \emph{hypermedia} can be used as an extension to hypertext which also includes other nonlinear medium of information (images, videos, etc.).
      Although the term hypermedia has been around for a long time, the term hypertext is still being used as synonym for hypermedia.
      Further, the web can also be referred to as the universe of information accessible through the web system.
      Therefore, the web is both the system enabling sharing of hypermedia and also all of the accessible hypermedia itself.
      Hypertext documents are today more known by the name \emph{web pages} or simply \emph{pages}.
      Multiple related pages compose a \emph{web site} or simply a \emph{site} and are usually hosted from the same domain.
      To transfer the resources between computers the protocol \textsc{http} is used.
      Typically the way of retrieving resources on the web is by using a \emph{web browser} or simply a \emph{browser}.
      Browsers handles the fetching, parsing and rendering of the hypertext (more about this in section~\ref{sec:browsers}).
      
      \section{The origin of the web}
        \begin{metatext}
          Since the web is a system operating on top of the Internet, it is needed to first investigate the origin of the Internet.
          This can be viewed from many angles and different aspects need to be taken into consideration.
          With that in mind, the origin of the Internet is not something easily pinned down and what will be presented here will be more of a technically interesting history.
          This section is a summary of \normalfont{\cite{overview_of_tcp_ip}\cite{internetsociety_history_internet}\cite{internet_maps}\cite{historyofthings_internet}}.
        \end{metatext}

        In the early 1960's \emph{packet switching} was being researched, which is a prerequisite of internetworking.
        With packet switching in place, the very important ancestor of the Internet \textsc{arpanet} (Advanced Research Projects Agency Network) was developed, which was the first network to implement the \textsc{tcp/ip} protocol suite.
        The \textsc{tcp/ip} protocol suite together with packet switching are fundamental technologies of the Internet.
        \textsc{arpanet} was funded by the United States Department of Defense (DoD) in order to interconnect their research sites in the United States.
        The first nodes of \textsc{arpanet} was installed at four major universities in the western United States in 1969 and two years later the network spanned the whole country.
        The first public demonstration of \textsc{arpanet} was held at the International Computer Communication Conference (ICCC) in 1972.
        It was also at this time the email system was introduced, which became the largest network application for over a decade.
        In 1973 the network had international connections to Norway and London via a sattelite link.
        At this time information was exchanged with the File Transfer Protocol (\textsc{ftp}), which is a protocol to transfer files between hosts.
        This can be viewed as the first generation of the Internet. With around 40 nodes, operating with raw file transfers between the hosts it was mostly used by the academic community of the United States.

        The number of nodes and hosts of \textsc{arpanet} increased slowly, mainly due to the fact that it was a centralized network owned and operated by the \textsc{us} military.
        In 1974 the \textsc{tcp/ip} stack was proposed in order to have a more robust and scalable system for end-to-end network communication.
        The \textsc{tcp/ip} stack is a key technology for the decentralization of the \textsc{arpanet}, to allow the massive expandation of the network that later happened.
        In 1983 \textsc{arpanet} switched to the \textsc{tcp/ip} protocols, and the network was split in two.
        One network was still called \textsc{arpanet} and was to be used for research and development sites.
        The other network was called \textsc{milnet} and was used for military purposes.
        The decentralization event was a key point and perhaps the birth of the Internet.
        The Computer Science Network (\textsc{csnet}) was funded by the National Science Foundation (\textsc{nsf}) in 1981 to allow networking benefits to academic insitutions that could not directly connect to \textsc{arpanet}.
        After the event of decentralizing \textsc{arpanet}, the two networks were connected among many other networks.
        In 1985 \textsc{nsf} started the National Science Foundation Network (nsfnet) program to promote advanced research and education networking in the \textsc{us}.
        To link the supercomputing centers funded by \textsc{nsf} the \textsc{nsfnet} serverd as a high speed and long distance backbone network.
        As more networks and sites were linked by the \textsc{nsfnet} network, it became the first backbone of the Internet.
        In 1992, around 6000 networks were connected to the \textsc{nsfnet} backbone with many international networks.
        To this point, the Internet was still a network for scientists, academic institutions and technology enthusiasts.
        Mainly, because \textsc{nsf} had stated that \textsc{nsfnet} was a network for non-commercial traffic only.
        In 1993 \textsc{nsf} decided to go back to funding research in supercomputing and high-speed communcations instead of funding and running the Internet backbone.
        That, along with an increasing preassure of commercializing the Internet let to another key event in the history of the Internet - the privatization of the \textsc{nsfnet} backbone.

        In 1994, the \textsc{nsfnet} was systematically privatized while making sure that no actor owned too much of the backbone in order to create constructive market competition.
        With the Internet decentralized and privatized regular people started using it as well as companies.
        Backbones were built across the globe, more international actors and organizations appeared and eventually the Internet as we know it today came to exist.

        \subsection{The World Wide Web}\label{sec:www}
          \begin{metatext}
            Now that the history of the Internet has been described, it is time to talk about the birth of the World Wide Web.
            Here the initial ideas of the web will be described, the alternatives and how it became a global standard.
            This subsection is a summary of \normalfont{\cite{wiki_gopher}\cite{wiki_www}\cite{webdevnotes_history_of_the_internet}\cite{webdevnotes_www_basics}\cite{historyofthings_internet}}.
          \end{metatext}

          Recall that the way of exchanging information was to upload and download files between clients and hosts with \textsc{ftp}.
          If a document downloaded was referring to another document, the user had to manually find the server that hosted the other document and download it manually.
          This was a poor way of digesting information and documents that linked to other resources.
          In 1989 a proposal for a communication system that allowed interlinked documents was submitted to the management at \textsc{cern}.
          The idea was to allow links embedded in text documents, to enable users to view the linked document by clicking it.
          A quote from the draft:
          \begin{quote}
            Imagine, then, the references in this document all being associated with the network address of the thing to which they referred, so that while reading this document you could skip to them with a click of the mouse.
          \end{quote}
          This catches the whole essence of the web in a sentence --- to interlink resources in an user friendly way.
          The proposal describes that such text embedded links would be hypertext.
          It continues to explain that interlinked resources does not need to be limited to text documents, multimedia such as images and videos can also be interlinked which would similarly be hypermedia.
          The concept of browsers is described, with a client-server model the browser would fetch the hypertext documents, parse them and handle the fetching of all media linked in the hypertext.

          In 1990, the Hypertext Transfer Protocol (\textsc{http}), the Hypertext Markup Language (\textsc{html}), a browser and a web server had been created and the web was born.
          One year later the web was introduced to the public and in 1993 over five hundred international web servers existed.
          It was stated in 1994 that the web was to be free without any patents or royalties.
          At this time the Wolrd Wide Web Consortium (\textsc{w3c}) was founded by Berners-Lee with support from the Defense Advanced Research Projects Agency and the European Commission.
          The organization comprised of companies and individuals that wanted to standardize and improve the web.

          As a side note, the Gopher protocol was developed in parallel to the World Wide Web by the University of Minnesota.
          It was released in 1991 and quickly gained traction as the web still was in very early stages.
          The goal of the system, just like the web, was to overcome the shortcomings of browsing documents with \textsc{ftp}.
          Gopher enabled servers to list the documents present, and also to link to documents on other servers.
          This created a strong hierarchy between the documents.
          The listed documents of a server could then be presented as hypertext menus to the client (much like a web browser).
          As the protocol was simpled than \textsc{http} it was often preferred since it used less network resources.
          The structure provided by Gopher provided a platform for large electronic library connections.
          A big difference between the web and the Gopher platform is that the Gopher platform provided hypertext menus presented as a file system while the web hypertext links inside hypertext documents, which provided greater flexibility.
          When the University of Minnesota announced that it would charge lincensing fees for the implementation, users were somewhat scared away.
          As the web matured, being a more flexible system with more features as well as being totally free it quickly became dominant.

      \section{The history of browsers}
        \label{sec:browsers}
        \begin{metatext}
          In the mid 1990's the usage of the Internet transitioned from downloading files with \textsc{ftp} to instead access resources with the \textsc{http} protocol.
          To fulfill the vision that users would be able to skip to the linked documents ``with a click of the mouse'' the users needed a client to handle the fetching and displaying of the hypertext documents, hence the need for browsers were apparent.
          Here the evolution of the browser clients will be given, while emphasizing the timeline of the popular browsers we use today.
          This section is a summary of \normalfont{\cite{wiki_www}\cite{wiki_nexus}\cite{wiki_lmb}\cite{wiki_libwww}\cite{wiki_mosaic}\cite{wiki_netscape}\cite{wiki_mozilla}\cite{wiki_opera}\cite{wiki_konqueror}\cite{wiki_safari}\cite{wiki_webkit}\cite{wiki_blink}}.
        \end{metatext}

        The first web browser ever made was created in 1990 and was called WorldWideWeb (which was renamed to Nexus to avoid confusion).
        It was at the time the only way to view the web, and the browser only worked on NeXT computers.
        Built with the NeXT framework, it was quite sophisticated.
        It had a \textsc{gui} and a \textsc{wysiwyg}\footnote{What You See Is What You Get is a classification that ensures that text and graphics during editing appears close to the result.} hypertext document editor.
        Unfortunately it couldn't be ported to other platforms, so a new browser called \emph{Line Mode Browser} (\textsc{lmb}) were quickly developed.
        To ensure compatability with the earliest computer terminals the browser displayed text, and was operated with text input.
        Since the browser was operated in the terminal, users could log in to a remote server and use the browser via telnet.
        The code that the two browsers shared was in 1993 bundled as a library called \emph{libwww}.
        The library was licensed as \emph{public domain} to encourage the development of web browsers.
        Many browsers were develop at this time.
        The \emph{Arena} browser served as a testbed browser and authoring tool for Unix.
        The \emph{ViolaWWW} browser was the first to support embedded scriptable objects, stylesheets and tables.
        \emph{Lynx} is a text based browser that supports many protocols (including Gopher and \textsc{http}), and is the oldest browser still being used and developed.
        The list of browsers of this time can be made long.

        In 1993, the \emph{Mosaic} browser was released by the National Center for Supercomputing Applications (\textsc{ncsa}) which came to be the ancestor of many of the popular browsers in use today.
        As Lynx, Mosaic also supported many different protocols.
        Mosaic quickly became popular, mainly due to the intuitive \textsc{gui}, reliability, simple installation and Windows compatability.
        The company \emph{Spyglass, Inc.} licensed the browser from \textsc{ncsa} for producing their own browser 1994.
        Around the same time the leader of the team that developed Mosaic, Marc Andreessen, left \textsc{ncsa} to start Mosaic Communications Corporation.
        The company released their own browser named \emph{Mosaic Netscape} in 1994, which later was to be called \emph{Netscape Navigator} which was internally codenamed \emph{Mozilla}.
        Microsoft licensed the Spyglass Mosaic browser in 1995, modified and renamed it to \emph{Internet Explorer}.
        In 1997 Microsoft started using their own \emph{Trident} layout engine for Internet Explorer.
        The norwegian telecommunications company \emph{Telenor} developed their own browser called \emph{Opera} in 1994, which was released 1996.
        Internet Explorer and Netscape Navigator were the two main browsers for many years, competing for market dominance.
        Netscape couldn't keep up with Microsoft, and was slowly losing market share.
        In 1998 Netscape started the open source Mozilla project, which made available the source code for their browser.
        Mozilla was to originally develop a suite of internet applications, but later switched focus to the \emph{Firefox} browser that had been created in 2002.
        Firefox uses the \emph{Gecko} layout engine developed by Mozilla.

        Another historically important browser is the \emph{Konqueror} browser developed by the free software community \textsc{kde}.
        The browser was released in 1998 and was bundled in the \textsc{kde} Software Compilation.
        Konqueror used the \textsc{kthml} layout engine, also developed by \textsc{kde}.
        In 2001, when \emph{Apple Inc.} decided to build their own browser to ship with \textsc{os x}, a fork called \emph{WebKit} was made of the \textsc{khtml} project.
        Apple's browser called \emph{Safari} was released in 2003.
        The WebKit form was made fully open source in 2005.
        In 2008, \emph{Google Inc.} also released a browser based on WebKit, named \emph{Google Chrome}, or shortened \emph{Chrome}.
        The majority of the source code for Chrome was open sourced as the \emph{Chromium} project.
        Google decided in 2013 create a fork of WebKit called \emph{Blink} for their browser.
        Opera Software decided in 2013 to base their new version of Opera on the Chromium project, using the Blink fork.

        \todo{Timeline image and text perhaps}


    \chapter{Web development}
      \begin{metatext}
        As the history of browsers has been presented, it is time to understand the evolution of web development.
        Browsers are the far most popular tools of accessing content on the web, which makes them very important in the modern society.
        In the dawn of the web, browsers were simply applications that fetched and displayed text with embedded links.
        Today, browsers act more like an operating system (on top of the host system), parsing and executing complex web applications.
        There even exist computers that only run a browser, which is sufficient for many users.
        This chapter will describe the transition from browsers rendering simple documents to being hosts for complex applications.
        It will also describe two key evolution points in web development --- responsive web design and modular development.
      \end{metatext}

      \todo{What to write here?}
      \section{From documents to applications}
        \begin{metatext}
          This section will describe the transition from browsers rendering simple documents to being hosts for complex applications.
          Since web development trends are not easily pinned to exact dates, this section will only present dates as guidance and should not be regarded as exact dates for the events.
          This section is a summary of \normalfont{\cite{wiki_web_dev}\cite{wiki_cgi}\cite{wiki_css}\cite{wiki_ajax}\cite{wiki_html5}\cite{book_html5}}.
        \end{metatext}

        As described in section~\ref{sec:www}, browsers initially were applications that displayed hypertext documents with the ability to fetch linked documents in an user friendly way.
        Static content were written in \textsc{html}, which could include hyperlinks to other hypertext documents or hypermedia.
        Different stylesheet languages were being developed to enable the possibility of separating content styling with the content.
        In 1996 \textsc{w3c} officially recommended the Cascading Style Sheets (\textsc{css}), which came to be the preferred way of styling web content.
        Since \textsc{html} is only a markup language it is not possible to generate dynamic content, which was sufficient at the time \textsc{html} only was used for annotating links in research documents.

        The need for generated dynamic content grew bigger, and \textsc{ncsa} created the first draft of the Common Gateway Interface (\textsc{cgi}) in 1993, which provides an interface between the web server and the systems that generate content.
        \textsc{cgi} programs are usually referred to as scripts, since many of the popular \textsc{cgi} languages are script langauges.
        To generate dynamic content on the server is sometimes referred to as \emph{serverside scriping}.
        This enabled developers to generate dynamic websites, with different content for different users for instance.
        However, when the content had been delivered to the client (browser) it was still static content.
        There is no way for the server to change the content that the client has received, unless the client requests another document.

        Around 1996, client side scripting was born.
        The term Dynamic \textsc{html} (\textsc{dhtml}) was being used as an umbrella term for a collection of technologies used together to make pages interactive and animated, where client side scripting played a big role.
        Examples of things that were being done with \textsc{dhtml} are; refreshing the pages for the user so that new content is loaded, give feedback on user input without involving the server and animating content.
        Plugins also existed during this time, which teaches the browsers how to handle and execute programs. \emph{Java applets} and \emph{Flash} are examples of this.
        The user then had to download the Java runtime or Flash runtime on the computer.
        When the runtimes are present, the browser then executed the included Java applets or Flash in the web pages by using the external runtimes.
        This enabled web developers to use ``real'' programming languages.
        The drawback of this is that users first had to download the runtimes in order to run the programs embedded in pages.
        As client side scripting matured and standardized into \emph{JavaScript}, plugins slowly decreased in popularity.
        With the increase of smart devices (such as phones, televisions, cars, game consoles, etc.) which included browsers but limited third-party runtimes, plugins quickly went extinct.
        Today, web sites rarely use plugins.

        As JavaScript and \textsc{html} supported more features, websites turned into small applications with user sessions (using cookies) and user interfaces.
        Still, parts of the applications were defined as \textsc{html} pages, fetched from the server when navigating the site.
        When \emph{XMLHttpRequest} \textsc{api} was supported in the major browsers, pages no longer needed to reload the page to fetch new content as XMLHttpRequest enabled developers to perform asynchronous requests to the server without page reload.
        This opened up for \emph{Ajax} web development technique which became a popular way of communicating with servers ``in the background'' of the page.
        Developers pushed browsers and \textsc{html} to the limit when creating applications instead of documents which it was originally designed for.
        In a \textsc{w3c} meeting in 2004 it was proposed to extend \textsc{html} to allow the creation of web applications, which was rejected.
        \textsc{w3c} was criticized of not listening to the need of the industry, and some members of \textsc{w3c} left to create the Web Hypertext Application Technology Working Group (\textsc{whatwg}).
        \textsc{whatwg} started working on specifications to ease the development of web applications which came to be named \textsc{html5}.
        In 2006, \textsc{w3c} realized that \textsc{whatwg} were on the right track, and decided to start working on their own \textsc{html5} specification based on the \textsc{whatwg} version.
        \textsc{html5} is an evolutionary improvement process of \textsc{html}, which means that browsers are adding support as parts of the specification is finished.

        A new era of \textsc{api}'s and features came along with the \textsc{html5} standard, which truly enabled developers to create rich client side applications.
        \textsc{css3} was also recently released which also included many new features.
        Using \textsc{html5} together with \textsc{css3} developers could utilize advanced graphics programming, geolocation, cache storage, file system acccess, offline mode, and much more.

        \todo{Write that traditional applications are threatened by web applications, since the reach and availability of the web is superior to any other distribution platform. Also, no installation is required with web applications. Also, updates and patches can be applied to all users instantaneously and enforced.}
      \section{Responsive web design}
        \begin{metatext}
          Back in the days, web developers could make assumptions about the screen size of the users.
          Since typically only desktop computers with monitors accessed web sites they were designed for a minimum viewport dimension.
          If the dimension of the viewport was smaller than the supported one, the site would look broken.
          This was a valid approach in a time when tablets and smartphones were unheard of.
          Today, another approach is needed to ensure that sites function properly across a range of different devices and viewport dimensions.
          This section is a summary of \normalfont{\cite{book_rwd}\cite{wiki_rwd}}.
        \end{metatext}

        According to \emph{StatCounter}\footnote{Statistics are based on aggregate data collected by StatCounter on a sample exceeding 15 billion pageviews per month collected from across the StatCounter network of more than 3 million websites.}, 37\% of the web users are visiting sites on a mobile or tablet device.
        No longer is it valid to not support small screens.
        Furthermore, it is understood that sites need to be styled differently if they are visisted by touch devices or mouse-based devices (such as laptops or desktops).
        Since web developers were not ready for this rapid change of device properties, they resorted to using the same approach that they had done before --- assumptions about the user device, but this time the other way around.
        When a browser request a resource, an agent string is usually sent with the request to identify what kind of browser the user is using.
        By reading the agent string at server side, a mobile-friendly version of the site could be served if the user was sending mobile agent strings and the desktop version could be served otherwise.
        The mobile version would be designed for a maximum width, and the desktop would be designed for a minimum width.
        This was a natural reaction since no better techniques existed, but the approach has many flaws.
        First, the developer now have two versions of a site to maintain and develop in parallel.
        Second, this approach doesn't scale well with new devices entering the market. For instance, tablets are somewhere in the middle of mobile and laptops in size which would require another special version of the site.
        Third, the desktop site assumes that desktop users have big screens (which in most of the times is true).
        However, there is no guarantee that the browser viewport will be big just because the screen is big.
        Users might want to have multiple browser windows displayed at the same time on the screen, which would break the assumptions about the layout size avaiable for the site.
        Clearly, a better approach was needed.

        With the release of \textsc{css3} \emph{media queries} new possibilities opened up.
        Media queries enabled developers to write conditional style rules by media properties such as viewport dimension.
        By using media queries, elements can be styled in the following manner:
        \begin{lstlisting}[caption={The above \textsc{css} styles the body of the website blue if the viewport is less or equal to 600 pixels wide, and yellow otherwise.},captionpos=b]]
@media screen and (max-width: 600px) {
  body {
    background-color: blue;
  }
}

@media screen and (min-width: 601px) {
  body {
    background-color: yellow;
  }
}
        \end{lstlisting}
        This can be used to tailor a site for a specific medium or viewport size at runtime.
        The term \emph{Responsive Web Design} (\textsc{rwd}) refers to the approach of having a single site \emph{responding} to different media properties (mainly viewport size) at runtime to improve the user experience.
        With \textsc{rwd} it is no longer needed to maintain several versions of a site, instead the site adapts to the user medium and device.

      \section{Modularity}
        \begin{metatext}
          This section is a summary of \normalfont{\cite{book_modular_programming}\cite{book_eloq_js}}.
        \end{metatext}

        By creating modules that can be used in any context with well defined responsibilities and dependencies, developing applications is reduced to the task of simply configuring modules (to some extent) to work together which forms a bigger application.
        It is today possible to write the web client logic in a modular way in JavaScript.
        The desire of writing modular code can be shown by the popularity of frameworks that helps dividing up the client code into modules.
        The ever so popular frameworks \emph{Angular}, \emph{Backbone}, \emph{Ember}, \emph{Web Components}, \emph{requirejs}, \emph{Browserify}, \emph{Polymer}, \emph{React} and many more all have in common that they embrace coding modular components.
        Many of these frameworks also help with dividing the \textsc{html} up into modules, creating small packages of style, markup and code.

        \todo{Flesh this one out, with meta text.}

    \chapter{Modular development}
      \todo{Remove this chapter?}
      \section{Web Components}
  \part{Theory}
    \chapter{layout engines}
      \begin{metatext}
        Before diving into the theory of element queries, it is important to understand how layout engines work.
        Only the layout engines of the browser will be of importance, since element queries does not affect any other browser subsystem.
        This chapter will give a brief explanation of the layouting flow that the engines generally perform, and how content changes affects the engine.
        It will also cover some optimizations that are done and which parts of the layout process that can be done in parallel.
      \end{metatext}

      Browsers are complex applications that consists of many subsystems.

    \chapter{Element queries}
  \part{Third-party framework}
    \chapter{Analysis of approaches}
    \section{Current implementations}
    \chapter{API design}
    \chapter{Implementation}
  \part{Result}
    \chapter{Discussion}
  \printbibliography
    \appendix
    \addappheadtotoc
      \chapter{Resources}
        \section{Practical problem formulation document}\label{sec:problem-formulation}
          TODO: Should this be in the real document instead of appendix?          
        \section{Practical cyclic rules discussion document}\label{sec:cyclic-rules}
          TODO: Should this be in the real document instead of appendix?
\end{document}
